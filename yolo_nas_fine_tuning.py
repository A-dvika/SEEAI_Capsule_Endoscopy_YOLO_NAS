# -*- coding: utf-8 -*-
"""YOLO_NAS_FINE_TUNING.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14JCtE7L-l1744cmsx-m1wTfvix-FMA_N
"""

pip install super-gradients==2.5.0

import sys
sys.path.append('C:/Users/HP/super_gradients/super_gradients')















pip install onnxruntime

import sys
sys.path.append('C:/Users/HP/AppData/Local/Programs/Python/Python310/Lib/site-packages')

import super_gradients

pip install super-gradients

from super_gradients.training import Trainer
from super_gradients.training import dataloaders
from super_gradients.training.dataloaders.dataloaders import (
    coco_detection_yolo_format_train,
    coco_detection_yolo_format_val
)

from super_gradients.training import models
from super_gradients.training.losses import PPYoloELoss
from super_gradients.training.metrics import (
    DetectionMetrics_050,
    DetectionMetrics_050_095
)


from super_gradients.training.models.detection_models.pp_yolo_e import PPYoloEPostPredictionCallback
from tqdm.auto import tqdm

import os
import requests
import zipfile
import cv2
import matplotlib.pyplot as plt
import glob
import numpy as np
import random

import super_gradients.training.utils.distributed_training_utils as distributed_training_utils

import os
import random
import shutil

# Set the root directory where the dataset is located
dataset_root = "C:\\Users\\HP\\Documents\\Untitled Folder"
# Set the directories for the training, validation, and testing sets
train_dir = "C:/Users/HP/Documents/Untitled Folder/train"
val_dir = "C:/Users/HP/Documents/Untitled Folder/val"
test_dir = "C:/Users/HP/Documents/Untitled Folder/test"

# Set the desired train/validation/test split ratio
train_ratio = 0.8  # 80% for training
val_ratio = 0.1    # 10% for validation
test_ratio = 0.1   # 10% for testing

# Create the directories for the train/validation/test sets
os.makedirs(train_dir, exist_ok=True)
os.makedirs(val_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

# Get the list of image files in the dataset directory
image_files = [filename for filename in os.listdir(os.path.join(dataset_root, "SEE_AI_project_all_images")) if filename.endswith(".jpg")]

# Shuffle the image files randomly
random.shuffle(image_files)

# Split the image files into train/validation/test sets based on the ratios
num_samples = len(image_files)
num_train = int(train_ratio * num_samples)
num_val = int(val_ratio * num_samples)

train_files = image_files[:num_train]
val_files = image_files[num_train:num_train+num_val]
test_files = image_files[num_train+num_val:]

# Move the image files and their corresponding annotation files to the train directory
for filename in train_files:
    image_path = os.path.join(dataset_root, "SEE_AI_project_all_images", filename)
    label_path = os.path.join(dataset_root, "SEE_AI_project_all_txt", filename.replace(".jpg", ".txt"))
    shutil.copy(image_path, os.path.join(train_dir, filename))
    shutil.copy(label_path, os.path.join(train_dir, filename.replace(".jpg", ".txt")))

# Move the image files and their corresponding annotation files to the validation directory
for filename in val_files:
    image_path = os.path.join(dataset_root, "SEE_AI_project_all_images", filename)
    label_path = os.path.join(dataset_root, "SEE_AI_project_all_txt", filename.replace(".jpg", ".txt"))
    shutil.copy(image_path, os.path.join(val_dir, filename))
    shutil.copy(label_path, os.path.join(val_dir, filename.replace(".jpg", ".txt")))

# Move the image files and their corresponding annotation files to the test directory
for filename in test_files:
    image_path = os.path.join(dataset_root, "SEE_AI_project_all_images", filename)
    label_path = os.path.join(dataset_root, "SEE_AI_project_all_txt", filename.replace(".jpg", ".txt"))
    shutil.copy(image_path, os.path.join(test_dir, filename))
    shutil.copy(label_path, os.path.join(test_dir, filename.replace(".jpg", ".txt")))

# Define the classes based on the annotation types
classes = ['angiodysplasia', 'erosion', 'stenosis', 'lymphangiectasia', 'lymph follicle', 'SMT',
           'polyp-like', 'bleeding', 'diverticulum', 'erythema', 'foreign body', 'vein']

# Define the dataset parameters
dataset_params = {
    'data_dir': dataset_root,
    'train_images_dir': train_dir,
    'train_labels_dir': os.path.join(train_dir, "annotations"),
    'val_images_dir': val_dir,
    'val_labels_dir': os.path.join(val_dir, "annotations"),
    'test_images_dir': test_dir,
    'test_labels_dir': os.path.join(test_dir, "annotations"),
    'classes': classes
}

pip install opencv-python

# Global parameters.
EPOCHS = 50
BATCH_SIZE = 16
WORKERS = 8

colors = np.random.uniform(0, 255, size=(len(classes), 3))

# Function to convert bounding boxes in YOLO format to xmin, ymin, xmax, ymax.
def yolo2bbox(bboxes):
    xmin, ymin = bboxes[0]-bboxes[2]/2, bboxes[1]-bboxes[3]/2
    xmax, ymax = bboxes[0]+bboxes[2]/2, bboxes[1]+bboxes[3]/2
    return xmin, ymin, xmax, ymax

def plot_box(image, bboxes, labels):
    # Need the image height and width to denormalize
    # the bounding box coordinates
    height, width, _ = image.shape
    lw = max(round(sum(image.shape) / 2 * 0.003), 2)  # Line width.
    tf = max(lw - 1, 1) # Font thickness.
    for box_num, box in enumerate(bboxes):
        x1, y1, x2, y2 = yolo2bbox(box)
        # denormalize the coordinates
        xmin = int(x1*width)
        ymin = int(y1*height)
        xmax = int(x2*width)
        ymax = int(y2*height)

        p1, p2 = (int(xmin), int(ymin)), (int(xmax), int(ymax))

        class_name = classes[int(labels[box_num])]

        color=colors[classes.index(class_name)]

        cv2.rectangle(
            image,
            p1, p2,
            color=color,
            thickness=lw,
            lineType=cv2.LINE_AA
        )

        # For filled rectangle.
        w, h = cv2.getTextSize(
            class_name,
            0,
            fontScale=lw / 3,
            thickness=tf
        )[0]

        outside = p1[1] - h >= 3
        p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3

        cv2.rectangle(
            image, p1, p2,
            color=color,
            thickness=-1,
            lineType=cv2.LINE_AA
        )
        cv2.putText(
            image,
            class_name,
            (p1[0], p1[1] - 5 if outside else p1[1] + h + 2),
            cv2.FONT_HERSHEY_SIMPLEX,
            fontScale=lw/3.5,
            color=(255, 255, 255),
            thickness=tf,
            lineType=cv2.LINE_AA
        )
    return image

# Function to plot images with the bounding boxes.
def plot(image_path, label_path, num_samples):
    all_training_images = glob.glob(image_path+'/*')
    all_training_labels = glob.glob(label_path+'/*')
    all_training_images.sort()
    all_training_labels.sort()

    temp = list(zip(all_training_images, all_training_labels))
    random.shuffle(temp)
    all_training_images, all_training_labels = zip(*temp)
    all_training_images, all_training_labels = list(all_training_images), list(all_training_labels)

    num_images = len(all_training_images)

    if num_samples == -1:
        num_samples = num_images

    plt.figure(figsize=(15, 12))
    for i in range(num_samples):
        image_name = all_training_images[i].split(os.path.sep)[-1]
        image = cv2.imread(all_training_images[i])
        with open(all_training_labels[i], 'r') as f:
            bboxes = []
            labels = []
            label_lines = f.readlines()
            for label_line in label_lines:
                label, x_c, y_c, w, h = label_line.split(' ')
                x_c = float(x_c)
                y_c = float(y_c)
                w = float(w)
                h = float(h)
                bboxes.append([x_c, y_c, w, h])
                labels.append(label)
        result_image = plot_box(image, bboxes, labels)
        plt.subplot(2, 2, i+1) # Visualize 2x2 grid of images.
        plt.imshow(image[:, :, ::-1])
        plt.axis('off')
    plt.tight_layout()
    plt.show()

# Visualize a few training images.
plot(
    image_path=os.path.join(ROOT_DIR, train_imgs_dir),
    label_path=os.path.join(ROOT_DIR, train_labels_dir),
    num_samples=4,
)

"""# Dataset Preparation for Training YOLO NAS"""

train_data = coco_detection_yolo_format_train(
    dataset_params={
        'data_dir': dataset_params['data_dir'],
        'images_dir': dataset_params['train_images_dir'],
        'labels_dir': dataset_params['train_labels_dir'],
        'classes': dataset_params['classes']
    },
    dataloader_params={
        'batch_size':BATCH_SIZE,
        'num_workers':WORKERS
    }
)

val_data = coco_detection_yolo_format_val(
    dataset_params={
        'data_dir': dataset_params['data_dir'],
        'images_dir': dataset_params['val_images_dir'],
        'labels_dir': dataset_params['val_labels_dir'],
        'classes': dataset_params['classes']
    },
    dataloader_params={
        'batch_size':BATCH_SIZE,
        'num_workers':WORKERS
    }
)

train_data.dataset.transforms

train_data.dataset.transforms[0]

train_data.dataset.transforms.pop(2)

train_data.dataset.transforms

train_data.dataset.plot(plot_transformed_data=True)

"""# The YOLO NAS Training Parameters"""

train_params = {
    'silent_mode': False,
    "average_best_models":True,
    "warmup_mode": "linear_epoch_step",
    "warmup_initial_lr": 1e-6,
    "lr_warmup_epochs": 3,
    "initial_lr": 5e-4,
    "lr_mode": "cosine",
    "cosine_final_lr_ratio": 0.1,
    "optimizer": "Adam",
    "optimizer_params": {"weight_decay": 0.0001},
    "zero_weight_decay_on_bias_and_bn": True,
    "ema": True,
    "ema_params": {"decay": 0.9, "decay_type": "threshold"},
    "max_epochs": EPOCHS,
    "mixed_precision": True,
    "loss": PPYoloELoss(
        use_static_assigner=False,
        num_classes=len(dataset_params['classes']),
        reg_max=16
    ),
    "valid_metrics_list": [
        DetectionMetrics_050(
            score_thres=0.1,
            top_k_predictions=300,
            num_cls=len(dataset_params['classes']),
            normalize_targets=True,
            post_prediction_callback=PPYoloEPostPredictionCallback(
                score_threshold=0.01,
                nms_top_k=1000,
                max_predictions=300,
                nms_threshold=0.7
            )
        ),
        DetectionMetrics_050_095(
            score_thres=0.1,
            top_k_predictions=300,
            num_cls=len(dataset_params['classes']),
            normalize_targets=True,
            post_prediction_callback=PPYoloEPostPredictionCallback(
                score_threshold=0.01,
                nms_top_k=1000,
                max_predictions=300,
                nms_threshold=0.7
            )
        )
    ],
    "metric_to_watch": 'mAP@0.50:0.95'
}

models_to_train = [
    'yolo_nas_s',
    'yolo_nas_m',
    'yolo_nas_l'
]

CHECKPOINT_DIR = 'checkpoints'

for model_to_train in models_to_train:
    trainer = Trainer(
        experiment_name=model_to_train,
        ckpt_root_dir=CHECKPOINT_DIR
    )

    model = models.get(
        model_to_train,
        num_classes=len(dataset_params['classes']),
        pretrained_weights="coco"
    )

    trainer.train(
        model=model,
        training_params=train_params,
        train_loader=train_data,
        valid_loader=val_data
    )

models_to_train = [
    'yolo_nas_s',
    'yolo_nas_m',
    'yolo_nas_l'
]

CHECKPOINT_DIR = 'checkpoints'

"""# Model Training"""

for model_to_train in models_to_train:
    trainer = Trainer(
        experiment_name=model_to_train,
        ckpt_root_dir=CHECKPOINT_DIR
    )

    model = models.get(
        model_to_train,
        num_classes=len(dataset_params['classes']),
        pretrained_weights="coco"
    )

    trainer.train(
        model=model,
        training_params=train_params,
        train_loader=train_data,
        valid_loader=val_data
    )



torch.save(model.state_dict(), 'C:/Users/HP/Documents/Untitled Folder/trained model')

model = torch.load('C:/Users/HP/Documents/Untitled Folder/ trained model')
model.eval()

input_image = cv2.imread('path/to/input_image.jpg')

# Run the inference
predictions = run_inference(input_image)

# Visualize the results
for box, class_label, confidence in predictions:
    xmin, ymin, xmax, ymax = box
    cv2.rectangle(input_image, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 2)
    cv2.putText(input_image, f"{class_label}: {confidence:.2f}", (int(xmin), int(ymin) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

# Display the image with bounding boxes and class labels
cv2.imshow("Inference Results", input_image)
cv2.waitKey(0)
cv2.destroyAllWindows()